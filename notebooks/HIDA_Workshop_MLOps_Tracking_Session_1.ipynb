{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PhEDIK1yAck"
      },
      "source": [
        "# HIDA Workshop Introduction to MLOps, Workflow tools for Data Science\r\n",
        "## Session 1: Tracking metrics, parameters, metadata and models\r\n",
        "\r\n",
        "***Christian Gerloff - Helmholtz School for Data Science in Life, Earth and Energy*** <br>\r\n",
        "This notebook consists of the practical examples of the first part of the workshop in MLOps and Workflow tools. <br><br><br>\r\n",
        "### Get started with the course materials\r\n",
        "To interactively work with the materials, you can open this notebook in  [google colab](https://https://colab.research.google.com/). All you need is a google account. Besides the server application, all course materials are prepared for direct use in google colab. No local installations are required. In the readme and during the course, we will provide you with an additional  how-to for local or remote installations. <br><br>\r\n",
        "\r\n",
        "### Credentials for cloud-hosted servers and storage\r\n",
        "\r\n",
        "To allow interaction during the workshop and to provide a realistic server setup for labs or industrial use-cases, we will use a cloud-hosted storage and mlflow server. Both are protected. Every participant will receive his/her own credentials for the mlflow serer via mail beforehand. The credentials are used to avoid collisions between runs so please use your own credentials. You should have received:\r\n",
        "\r\n",
        "* MLFLOW_TRACKING_USERNAME\r\n",
        "* MLFLOW_TRACKING_PASSWORD\r\n",
        "* AWS_ACCESS_KEY_ID\r\n",
        "* AWS_SECRET_ACCESS_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNN-EKoNaqFJ"
      },
      "source": [
        "## 1 Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaPOCy9PNcRW"
      },
      "source": [
        "### 1.1 Install required packages\n",
        "Here we install the required packages. <br><br>\n",
        "\n",
        " ***Tip: random initialization***: Several methods that we apply or develop in Data Science rely on some introduced randomness. As we work today on deterministic machines, these numbers are not entirely random; instead, their generation procedure depends on a seed. Hence, today the random numbers we use are deterministically specified, often via the CPU clock time. This has disadvantages but brings one advantage; We can set a seed to enable reproducible results. We suggest to always, always set a seed. Moreover, we recommend testing the variance introduced by different seeds because your final conclusions in your paper should not depend on the seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOy4Tn1kbNds"
      },
      "outputs": [],
      "source": [
        "!pip -q install mlflow boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwl87IbXrc4E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import mlflow as mf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as clr\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "from joblib import load\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# set seed\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vusy8dHPpOe6"
      },
      "source": [
        "### 1.2 Gather data sets from AWS S3\n",
        "The original datasets are usually stored ***locally***, on an HDFS system, FDP, DBs( e.g., postgres) or cloud storage such as S3 (see storages 3.1). For demonstration purposes, we load the initial data set from S3 via boto. The code is not so important if you work locally. \n",
        "\n",
        "***Tip: data consistency:*** We recommend that you never modify the initial dataset. Always create a new version of your dataset when you touch it. If you need to save the manipulated data for subsequent analyses, a multi-stage pipeline may be useful (see session 2 of the workshop). An excellent alternative to ensure consistent data is the data versioning system `datalad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvXKZUFHbg4c"
      },
      "outputs": [],
      "source": [
        "# aws settings for raw data & artifact storage\r\n",
        "os.environ['AWS_ACCESS_KEY_ID'] =\r\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] =\r\n",
        "BUCKET_NAME = 'hida-workshop-data'\r\n",
        "AWS_ACC_KEY = os.getenv('AWS_ACCESS_KEY_ID')\r\n",
        "AWS_SEC_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\r\n",
        "\r\n",
        "# specify aws resources to gather intial data\r\n",
        "client = boto3.client('s3', aws_access_key_id=AWS_ACC_KEY, \r\n",
        "                      aws_secret_access_key=AWS_SEC_KEY)\r\n",
        "s3 = boto3.resource('s3')\r\n",
        "\r\n",
        "def download_s3(client, resource, bucket, dist, local='/tmp'):\r\n",
        "    paginator = client.get_paginator('list_objects')\r\n",
        "    for result in paginator.paginate(Bucket=bucket, Delimiter='/', Prefix=dist):\r\n",
        "        if result.get('CommonPrefixes') is not None:\r\n",
        "            for subdir in result.get('CommonPrefixes'):\r\n",
        "              download_s3(client, resource, bucket, subdir.get('Prefix'), local)\r\n",
        "        for file in result.get('Contents', []):\r\n",
        "            dest_pathname = os.path.join(local, file.get('Key'))\r\n",
        "            if not os.path.exists(os.path.dirname(dest_pathname)):\r\n",
        "                os.makedirs(os.path.dirname(dest_pathname))\r\n",
        "            if file.get('Key')[-1] != \"/\":\r\n",
        "              resource.meta.client.download_file(bucket, file.get('Key'), dest_pathname)\r\n",
        "\r\n",
        "download_s3(client, s3, BUCKET_NAME, 'fetal_health', 'data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrShsi9gaoqd"
      },
      "source": [
        "\n",
        "## 2 A first data analysis \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TPh5pigNsj5"
      },
      "source": [
        "### 2.1 Explorative data analysis\r\n",
        "***Description:*** Data Set | Classification task\r\n",
        "\r\n",
        "Cardiotocograms (CTGs) are a simple and cost accessible option to assess fetal health, allowing healthcare professionals to take action in order to prevent child and maternal mortality. The equipment itself works by sending ultrasound pulses and reading its response, thus shedding light on fetal heart rate (FHR), fetal movements, uterine contractions and more. <br><br>\r\n",
        "\r\n",
        "***Reference***: Ayres-de-Campos, D., Bernardes, J., Garrido, A., Marques-de-Sa, J., & Pereira-Leite, L. (2000). SisPorto 2.0: a program for automated analysis of cardiotocograms. Journal of Maternal-Fetal Medicine, 9(5), 311-318."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38YsfwdCf7Zb"
      },
      "outputs": [],
      "source": [
        "# load data set perform some EDA\n",
        "data = pd.read_csv(Path.cwd() / 'data/fetal_health/fetal_health.csv')\n",
        "\n",
        "labels = data.fetal_health.astype(int)\n",
        "features = data.drop(columns=['fetal_health'])\n",
        "\n",
        "fig = plt.figure(figsize=(20,15))\n",
        "plt.suptitle(\"Distribution of the Numeric variables\", weight='bold', y=1.1)\n",
        "for i, f in enumerate(features.columns):\n",
        "    ax=plt.subplot(7,3,1+i)\n",
        "    ax=sns.kdeplot(data=features, x=f, fill=True, alpha=1)\n",
        "    ax.set_title(f,y=1.1)\n",
        "plt.tight_layout(pad=0, w_pad=2, h_pad=2)\n",
        "\n",
        "# corr\n",
        "fig=plt.figure(figsize=(15,8))\n",
        "sns.heatmap(features.corr(), linewidths=3, annot=True)\n",
        "plt.title(\"Correlation matrix\", size=20, weight='bold')\n",
        "\n",
        "# labels\n",
        "fig=plt.figure(figsize=(15,8))\n",
        "sns.histplot(data=labels, discrete=True)\n",
        "plt.title(\"Dependent variable\", size=20, weight='bold')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQVQI6LDsr0Y"
      },
      "source": [
        "#### Additional notes\n",
        "\n",
        "N = 2126 records \n",
        "\n",
        "Classes 'fetal_health':\n",
        "* Normal == 1\n",
        "* Suspect == 2\n",
        "* Pathological == 3\n",
        "\n",
        "Features:\n",
        "\n",
        "* 'baseline value' FHR baseline (beats per minute)\n",
        "* 'accelerations' Number of accelerations per second\n",
        "* 'fetal_movement' Number of fetal movements per second\n",
        "+ 'uterine_contractions' Number of uterine contractions per second\n",
        "* 'light_decelerations' Number of light decelerations per second\n",
        "* 'severe_decelerations' Number of severe decelerations per second\n",
        "* 'prolongued_decelerations' Number of prolonged decelerations per second\n",
        "* 'abnormal_short_term_variability' Percentage of time with abnormal short term variability\n",
        "* 'mean_value_of_short_term_variability' Mean value of short term variability\n",
        "* 'percentage_of_time_with_abnormal_long_term_variability' Percentage of time with abnormal long term variability\n",
        "* 'mean_value_of_long_term_variability' Mean value of long term variability\n",
        "* 'histogram_width' Width of FHR histogram\n",
        "* 'histogram_min' Minimum (low frequency) of FHR histogram\n",
        "* 'histogram_max' Maximum (high frequency) of FHR histogram\n",
        "* 'histogram_number_of_peaks' Number of histogram peaks\n",
        "* 'histogram_number_of_zeroes' Number of histogram zeros\n",
        "* 'histogram_mode' Histogram mode\n",
        "* 'histogram_mean' Histogram mean\n",
        "* 'histogram_median' Histogram median\n",
        "* 'histogram_variance' Histogram variance\n",
        "* 'histogram_tendency' Histogram tendency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MygJ-GPpk2O"
      },
      "source": [
        "### 2.2 A simple prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFRmEP8QnqxI"
      },
      "outputs": [],
      "source": [
        "# create train set for cross-validation and additional hold-out set\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    features,labels,test_size=0.3, stratify=labels, random_state=SEED)\n",
        "\n",
        "# create a simple sklearn pipeline\n",
        "pipe = Pipeline(steps=[('sts', StandardScaler()),\n",
        "                       ('cls', RidgeClassifier(alpha=1.0, random_state=SEED))])\n",
        "\n",
        "# cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "scores = cross_val_score(pipe,\n",
        "                         x_train,\n",
        "                         y_train,\n",
        "                         cv=cv,\n",
        "                         scoring='balanced_accuracy')\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.displot(scores, kde=True)\n",
        "plt.xlim(scores.min(), 1)\n",
        "plt.title(\"ACC\", size=20, weight='bold')\n",
        "\n",
        "# test on hold-out\n",
        "pipe.fit(x_train, y_train)\n",
        "est_y = pipe.predict(x_test)\n",
        "\n",
        "acc = metrics.balanced_accuracy_score(y_test, est_y)\n",
        "precision, recall, f_score, support = metrics.precision_recall_fscore_support(\n",
        "        y_test, est_y, beta=2, average=None)\n",
        "\n",
        "print('Performance CV \\n'\n",
        "      f'acc mean: {np.mean(scores):.2f} \\n'\n",
        "      f'acc std: {np.std(scores):.2f} \\n \\n'\n",
        "      'Hold-out: one-vs-rest performance \\n'\n",
        "      f'acc : {acc:.2f} \\n'\n",
        "      f'precision: {precision} \\n'\n",
        "      f'recall: {recall} \\n'\n",
        "      f'f-score: {f_score} \\n'\n",
        "      f'support: {support} \\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eviXALM3J08"
      },
      "source": [
        "***Motivation:*** But what if we want to compare several models*?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsSdlw25B10N"
      },
      "source": [
        "## 3 Setting up MLflow for tracking\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAFCPreuChps"
      },
      "source": [
        "\r\n",
        "### 3.1 Backround: What can we take off in the DataScience Lifecycle via MLflow and where is it stored?\r\n",
        "<br><br>\r\n",
        "\r\n",
        "MLFlow is backed by two storages. One storage to so store files/artifacts and one storage to store all meta data.\r\n",
        "\r\n",
        "### Tracking / Metadata stores (your experiment)\r\n",
        "\r\n",
        "* Local file path where data is directly stored locally on your machine.`mlruns/`\r\n",
        "*   Mlflow supports the several DBs such as mysql, sqlite or postgresql. To connect to the data base have a look at your prefaired ODM for the corresponding data base type (e.g., sqlalchemy for postgres)\r\n",
        "* HTTP server, which is a server hosting an MLFlow tracking server.\r\n",
        "    * locally: https://my-server:5000\r\n",
        "    * **In this workshop it is hosted remote**\r\n",
        "\r\n",
        "***Note:*** Alternatively you could also use Databricks workspace.\r\n",
        "<br><br>\r\n",
        "\r\n",
        "    \r\n",
        "### Artifact stores (Every data that specifically belongs to a run)\r\n",
        "\r\n",
        "The default artifact store is your local folder which is feasible for local installations of MLflow. For this workshop, we hosted an MLflow server in the cloud with S3 as an artifact store.\r\n",
        "Store Options:\r\n",
        "* Amazon S3\r\n",
        "* Azure Blob Storage\r\n",
        "* Google Cloud Storage\r\n",
        "* FTP server\r\n",
        "* SFTP Server\r\n",
        "* NFS\r\n",
        "* HDFS\r\n",
        "<br><br>\r\n",
        "\r\n",
        "### ***Run***\r\n",
        "An instnace of code that is tracked via MLflow. It can contain several, such as tags, notes, parameters, artifacts.\r\n",
        "<br><br>\r\n",
        "### ***Experiment***\r\n",
        "Is a set of runs. For example your current research project.\r\n",
        "<br><br>\r\n",
        "### ***Tags & Notes*** (stored in metadata storage): string\r\n",
        "Information about a run, such as its main aim and difference or core assumption,\r\n",
        "underling data set name\r\n",
        "\r\n",
        "Notes can be added per run or for an experiment. They support markdown.\r\n",
        "```\r\n",
        "MlflowClient().set_tag(run_id, \r\n",
        "     \"mlflow.note.content\",\"***<nice_note>***\")\r\n",
        "clieMlflowClient().set_experiment_tag(experiment_id, \r\n",
        "     \"mlflow.note.content\",\"***<nice_note>***\")\r\n",
        "```\r\n",
        "<br><br>\r\n",
        "### ***Parameters*** (stored in metadata storage): e.g. dict\r\n",
        "Key-value inputs for your code & model\r\n",
        "\r\n",
        "```\r\n",
        "parameters = {'s_width':  10}  # slinding window width in seconds\r\n",
        "MlflowClient().log_params(parameters)\r\n",
        "```\r\n",
        "<br><br>\r\n",
        "### ***Metrics*** (stored in metadata storage): e.g. int or float\r\n",
        "Numeric values, can contain temporal dependencies\r\n",
        "```\r\n",
        "MlflowClient().log_metrics({'ROC_AUC': 90.6}, step=1)\r\n",
        "```\r\n",
        "<br><br>\r\n",
        "### ***Artifact*** (stored in artifact storage)\r\n",
        "Resulting data, such as preprocessed data, modes, figures or other files\r\n",
        "<br><br>\r\n",
        "### ***Source information & Models*** (stored in artifact storage)\r\n",
        "MLflow can store models, their version and the source of the associated code (git hashes only).\r\n",
        "Everything can be defined manually, but for simple use cases autotracking can be considered. Integration with git is not as feasible as in DVC and this is a main drawback of MLflow from our point of view. We will discuss source information and models in session 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk8YlpvkiiD7"
      },
      "source": [
        "### 3.2 Manual tracking \n",
        "\n",
        "Here we specify where our MLflow server is located. If you are running MLflow locally, you don't have to worry about the tracking URL.\n",
        "The artifact storage and metadata storage are already configured. We only have to specify the name of the experiment and our username."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXmCNzpnrJRe"
      },
      "outputs": [],
      "source": [
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \r\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \r\n",
        "\r\n",
        "MF_URL = \r\n",
        "EXPERIMENT = 'Examples-Session-1'\r\n",
        "NOTE_EXPERIMENT = 'This experiment belongs to the workshop Session 1'\r\n",
        "\r\n",
        "mf.set_tracking_uri(MF_URL)\r\n",
        "mf.set_experiment(experiment_name=EXPERIMENT)\r\n",
        "experiment = mf.get_experiment_by_name(name=EXPERIMENT)\r\n",
        "\r\n",
        "client = MlflowClient()\r\n",
        "client.set_experiment_tag(experiment.experiment_id, \r\n",
        "     \"mlflow.note.content\",NOTE_EXPERIMENT)\r\n",
        "\r\n",
        "params = {'seed': SEED,\r\n",
        "          'test_size': 0.3,\r\n",
        "          'k_cv': 5,\r\n",
        "          'suffle_cv': True,\r\n",
        "          'alpha': 0.9}\r\n",
        "\r\n",
        "tags = {'data': 'raw_fetal_data',\r\n",
        "        'objective': 'influence_of_splits'}\r\n",
        "\r\n",
        "NOTE = 'My first manually ***tracked classification***'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAhrZG9T0gAm"
      },
      "outputs": [],
      "source": [
        " # create additional hold-out set\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "     features,\n",
        "     labels,\n",
        "     test_size=params['test_size'],\n",
        "     stratify=labels,\n",
        "     random_state=params['seed'])\n",
        " \n",
        "cv = StratifiedKFold(n_splits=params['k_cv'],\n",
        "                     shuffle=True,\n",
        "                     random_state=params['seed'])\n",
        "# pipeline\n",
        "stages = [('sts', StandardScaler()),\n",
        "          ('cls', RidgeClassifier(alpha=params['alpha'],\n",
        "                      random_state=params['seed']))]\n",
        "pipe = Pipeline(steps=stages)\n",
        "\n",
        "#start with the manual tracking\n",
        "with mf.start_run() as run:\n",
        "\n",
        "  # log meta data & parameters\n",
        "  mf.set_tags(tags)  # add tags, e.g. to filter runs\n",
        "  mf.set_tag('mlflow.note.content', NOTE)  # add notes\n",
        "  mf.set_tag('mlflow.user', os.getenv('MLFLOW_TRACKING_USERNAME'))  # add user name\n",
        "  mf.log_params(params)\n",
        "\n",
        "  # log performance of the cross-validation on train set\n",
        "  scores = cross_val_score(pipe,\n",
        "                           x_train,\n",
        "                           y_train,\n",
        "                           cv=cv,\n",
        "                           scoring='balanced_accuracy')\n",
        "\n",
        "  for i, s in enumerate(scores):\n",
        "        mf.log_metrics({'training_accuracy_score': s}, step=i+1)\n",
        "\n",
        "  # train model on train set\n",
        "  pipe.fit(x_train, y_train)\n",
        "  est_y = pipe.predict(x_test)\n",
        "  \n",
        "  # test an log the performance on hold-out set\n",
        "  acc = metrics.balanced_accuracy_score(y_test, est_y)\n",
        "  mf.log_metrics({'test_accuracy_score': acc})\n",
        "  precision, *_ = metrics.precision_recall_fscore_support(\n",
        "          y_test, est_y, beta=2, average=None)\n",
        "  for i, p in enumerate(precision):\n",
        "    mf.log_metrics({'test_precision': p}, step=i+1)\n",
        "\n",
        "  # store model\n",
        "  mf.sklearn.log_model(pipe, 'model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAcofr89h5E2"
      },
      "source": [
        "***Note:*** Please do not forget to set the username in all your runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrXBGsgvzqwf"
      },
      "source": [
        "### 3.3 Automatic tracking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkzyDq9rI8ZY"
      },
      "outputs": [],
      "source": [
        "#create data set\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "     features,\n",
        "     labels,\n",
        "     test_size=0.3,\n",
        "     stratify=labels,\n",
        "     random_state=SEED)\n",
        " \n",
        "cv = StratifiedKFold(n_splits=5,\n",
        "                     shuffle=True,\n",
        "                     random_state=SEED)\n",
        "# pipeline\n",
        "stages = [('sts', StandardScaler()),\n",
        "          ('cls', RidgeClassifier(alpha=0.1,\n",
        "                      random_state=SEED))]\n",
        "\n",
        "pipe = Pipeline(steps=stages)\n",
        "pipe.fit(x_train, y_train) \n",
        "\n",
        "#enable autologging\n",
        "mf.sklearn.autolog(disable=False, silent=True)\n",
        "with mf.start_run() as run:\n",
        "\n",
        "  mf.set_tag('mlflow.user', os.getenv('MLFLOW_TRACKING_USERNAME'))\n",
        "  mf.set_tags(tags)\n",
        "\n",
        "  # test on hold-out\n",
        "  hold_out_metrics = mf.sklearn.eval_and_log_metrics(\n",
        "      pipe, x_test, y_test, prefix=\"test_\")\n",
        "  cross_val_score(pipe, x_train, y_train, cv=cv)\n",
        "\n",
        "# just for demonstration to ensure that autologging is off if you rerun a cell\n",
        "mf.sklearn.autolog(disable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-rnH7_W90e5"
      },
      "source": [
        "`mlflow.autolog` is an experimental feature that can already save you some code to specify what you want to log. It currently supports Pytorch, Tensorflow, and XGBoost to automatically common metrics, models, parameters, and input examples. Hence, the `params` dictionary becomes unnecessary here. <br><br>\n",
        "\n",
        "***Tip: autotracking***: For sklearn, parameters are always stored if `.fit`method  and its derivates (`.fit_transform`) are called. The corresponding metrics are stored with the prefix `training`. Moreover, the artifacts with the default names will be overwritten for each call. Be aware of that if you call fit multiple times, such as here. In the example of this notebook, we called the fit method for hold-out evaluation before the autotracking starts and stored the metrics and artifacts with the prefix `test_` to avoid this issue in autotracking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ3OPUKhaJmC"
      },
      "source": [
        "# 4 How to fetch your tracked data - The non-UI way\n",
        "\n",
        "An advantage of MLflow compared to DVC is its UI. The UI already provides an easy way to inspect your tracked data and to compare your results.\n",
        "\n",
        "Out-of-the-box UI features:\n",
        "* download of parameters and metrics as CSV across runs\n",
        "* comparison and comparison charts across runs\n",
        "* Detail graphs for each metric in a run\n",
        "\n",
        "Nevertheless, often we want to create tables and figures locally after we have carried out all the analyses and perhaps also saved the first result graphics as artifacts. Or we want to create a comprehensive report.\n",
        "For these and other scenarios, it is necessary to be able to extract the data from the individual runs via API. In the following, we will briefly give an example of this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPVeIucTxqYz"
      },
      "source": [
        "## 4.1 Get runs of an experiment\n",
        "To get all runs that your are interested in you can easaly filter runs by defining a filter `strings` for tags or parameters.\n",
        "\n",
        "Here we filter for user and data and additionally order the results.\n",
        "The results are already Ã²f type `pandas.DataFrame`which makes it streightforward to inspect and visualize your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzyq8J7taOm4"
      },
      "outputs": [],
      "source": [
        "# specify the experiment that we are interested in\r\n",
        "experiment = mf.get_experiment_by_name(name='Examples-Session-1')\r\n",
        "\r\n",
        "# specify a filter to select a subset of runs, e.g. ony our own runs\r\n",
        "filter = f\"tags.mlflow.user='{os.getenv('MLFLOW_TRACKING_USERNAME')}'\"\r\n",
        "\r\n",
        "# fetch the data\r\n",
        "runs = mf.search_runs(experiment_ids=experiment.experiment_id,\r\n",
        "                      filter_string=filter,\r\n",
        "                      order_by=['tags.mlflow.user',\r\n",
        "                                'tags.data',\r\n",
        "                                'metrics.test_accuracy_score'])\r\n",
        "runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsOZNcRLJzSr"
      },
      "source": [
        "***Tip: fetch runs:*** This method has one drawback. If you inspect the results, you will discover that your results from metrics with multiple values (or in mlflow termed `history`), such as the performance metrics of cross-validation, only show the last fold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCwciLpDHQij"
      },
      "outputs": [],
      "source": [
        "# show the missing fold specif information of this procedure\r\n",
        "runs['metrics.training_accuracy_score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziL2g5jq4uBN"
      },
      "source": [
        "Therefore, you can also gather the metadata of a specific run if you have initialized a client object `MlflowClient()`. This method provides you also with the history of a metric.\n",
        "\n",
        "\n",
        "For example, here we collect a dictionary of performance measures of the cross-validation only from a specific run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGLl2V7g4Vz_"
      },
      "outputs": [],
      "source": [
        "# fetch a nested metric of a specific run \r\n",
        "metric = client.get_metric_history(\r\n",
        "      runs.run_id[0], 'training_accuracy_score')\r\n",
        "print(f\"{metric[0].key}: \\n {[f'{i}: {v.value:.2f}' for i, v in enumerate(metric)]}\")\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrSp18IELLD9"
      },
      "source": [
        "**Congratulations - you have achieved Data Science Ninja Level 1!** <br>\n",
        "Take a break, Session 2 will start soon."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HIDA-Workshop - Tracking.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "724d7f4835cb671c0cd3b6a80ea8ae7681ec35122d136ff0dffb937dc8baf58d"
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
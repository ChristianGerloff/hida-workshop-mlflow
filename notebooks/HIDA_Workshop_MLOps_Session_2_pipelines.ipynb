{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4OD6udD_s-p"
      },
      "source": [
        "# HIDA Workshop Introduction to MLOps, Workflow tools for Data Science\r\n",
        "## Session 2: Pipelines & MLFlow projects\r\n",
        "\r\n",
        "***Christian Gerloff - Helmholtz School for Data Science in Life, Earth and Energy*** <br>\r\n",
        "This notebook consists of the practical examples of the second part of the workshop in MLOps and Workflow tools. All course materials are prepared to run via google colab without further requirements<br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au5QlNrLyOS1"
      },
      "source": [
        "## 1 Preparation\n",
        "\n",
        "Here we download the MLFlow project and install miniconda in colab, which we need for the environment in which the project is about to run. By defining a specific conda environment or by setting up a Docker container, we aim for the technical reusability of the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi5RryZxx4kq",
        "outputId": "f40b8954-2340-4fb3-b2db-f24394138a48"
      },
      "outputs": [],
      "source": [
        "# donwload the project from the workshop repo\n",
        "!wget -O - https://github.com/ChristianGerloff/hida-workshop-mlflow/archive/refs/heads/mlfow-projects.tar.gz | tar xz \\\n",
        "       --strip=1 \"hida-workshop-mlflow-mlfow-projects/mlflow_projects\"\n",
        "\n",
        "# download miniconda\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# install required packages to the start the project\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.8/site-packages/')\n",
        "!pip install mlflow python-dotenv --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjg8EST6ysI5"
      },
      "source": [
        "## 2 Pipelines\n",
        "A pipeline can be basically described as sequences of functions. We could describe a pipeline $P$ as a set of functions $F$ and its sequential relation between each function $E$. To make the story short - a pipeline can be described as a directed acyclic graph. \n",
        "\n",
        "A pipeline can reach certain complexity - some guiding questions?:\n",
        "* How many steps has the pipeline\n",
        "* Is the state of the previous step in the pipeline relevant (synchronized / or not) \n",
        "* Does the pipeline depend on multiple frameworks/packages?\n",
        "* Do I have to run the pipeline from beginning to end or do I have multiple entry points?\n",
        "* How is the pipeline triggered (manually, automatically, via an event or as a cron job)?\n",
        "* How are the results of my pipleline served?\n",
        "* ....\n",
        "\n",
        "\n",
        "Hence, a pipeline can be realized in many ways (see previous discussion):\n",
        "1. Native sequence of functions or methods\n",
        "2. Packet/framework specific pipelines \n",
        "3. Airflow, DVC, MLFlow\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7vtsTR7yoM2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqTBkSuFvHg"
      },
      "source": [
        "## 3 From tracking scripts to small pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYxjGgxnJILS"
      },
      "source": [
        "### 3.1 Prepare credentials & environment variables\n",
        "The example project uses a `.env` file to store the credentials.\n",
        "Therefore, please upload a `.env` file consisting of all required environment variables or create it via the code below. Please be aware to add line breaks at the end of each variable `\"<env.var.name>=<value>\\n\",`\n",
        "\n",
        " ***Note***: In this colab setting you could also directly specify the environment variables in a code cell. In production, we highly suggest isolating the confidential environment variables in a file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_bT5p-MIZPg"
      },
      "outputs": [],
      "source": [
        "with open(\"mlflow_projects/.env\", \"w\") as f:\r\n",
        "    f.writelines([\"AWS_ACCESS_KEY_ID=\\n\",\r\n",
        "                  \"AWS_SECRET_ACCESS_KEY=\\n\",\r\n",
        "                  \"BUCKET_NAME=hida-workshop-data\\n\",\r\n",
        "                  \"MLFLOW_TRACKING_URL=http://3.125.220.21:80\\n\",\r\n",
        "                  \"MLFLOW_TRACKING_USERNAME=\\n\",\r\n",
        "                  \"MLFLOW_TRACKING_PASSWORD=\\n\",\r\n",
        "                  \"MLFLOW_EXPERIMENT=Example-Session-2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf64Sumy_WN4"
      },
      "source": [
        "Now let's take a look at the pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQNK9D8XINQy"
      },
      "source": [
        "### 3.2 Start the example pipeline\n",
        "Here we change the working directory and trigger the pipeline via a starting script. This starting script will perform the following actions:\n",
        "\n",
        "* load environment variables from `.env`\n",
        "* setup the MLFLow client\n",
        "* prepare and activate the conda environment via the `conda.yaml`\n",
        "* start the pipelines with the defined entry point and parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlZXUgUbFw6P",
        "outputId": "52abe535-da2c-4588-843b-52e619c88366"
      },
      "outputs": [],
      "source": [
        "import os\r\n",
        "\r\n",
        "# change the current working directory\r\n",
        "os.chdir('/content/mlflow_projects')\r\n",
        "\r\n",
        "# start our project runs\r\n",
        "!python start.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7BovOGpbLu0"
      },
      "source": [
        "In the UI you will find two nested runs in the experiment. \n",
        "The first nested run contains all the steps in our pipeline, while the second contains only the last step. As discussed earlier, this behaviour results from our multistep setting. In this setting, we avoid repeating runs with identical data, parameters, etc. and only run parts of the pipeline that have changed or are new."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uiiw8Td70JT"
      },
      "source": [
        "### 3.3 Linting & Testing\n",
        "\n",
        "While it is best practice in modern software development to integrate test stages into CI/CD pipelines, analytical pipelines can also benefit from ensuring correct behavior and reliable results via testing strategies. Two possible options to do so are:\n",
        "\n",
        "\n",
        "1.   Integrate tests into each entry point of the pipeline,\n",
        "2.   Add a test(s) as a separate entry point into your pipeline.\n",
        "\n",
        "\n",
        "We prefer option (2), which is more in line with our software development practices. Therefore, we run the test entry point at the end of the pipelines and reset the state of the previous runs to `failed` if the tests were not successful. In this introductory workshop, we will not cover this topic further. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3dGqG7RMdSJ"
      },
      "source": [
        "### 3.4 How to submit your final results to the \"Leaderboard\"\n",
        "\n",
        "Here we manually submit the results. \n",
        "\n",
        "***Tip:*** Alternatively, you can write your own automated submission procedure. To do so take a look at session 1 for fetching and how we reused runs in main.py - perhaps it helps you :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2XgB98sMtIu",
        "outputId": "3a2e4f7a-2b1f-4527-ea46-d2d93ddae80b"
      },
      "outputs": [],
      "source": [
        "import mlflow as mf\r\n",
        "from dotenv import load_dotenv\r\n",
        "\r\n",
        "# specify your team name, dataset (ASD/Fetal) an run_id of your final run\r\n",
        "tags = {'team': 'HDS-LEE-Coffee-addicts',\r\n",
        "        'dataset': 'ASD'\r\n",
        "        'run_id' '12312312312312'}\r\n",
        "\r\n",
        "metric = {'test_accuracy_score': 0.222,\r\n",
        "          'test_f1_score': 0.12,\r\n",
        "          'test_precision_score': 0.1,\r\n",
        "          'test_recall_score': 0.3,\r\n",
        "          'test_roc_auc': 0.5}\r\n",
        "\r\n",
        "# feel free to add your parameters and model :)\r\n",
        "\r\n",
        "load_dotenv()\r\n",
        "mf.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URL'))\r\n",
        "mf.set_experiment('Leaderboard')\r\n",
        "mf.set_tag('mlflow.user', os.getenv('MLFLOW_TRACKING_USERNAME'))\r\n",
        "\r\n",
        "with mf.start_run() as run:\r\n",
        "  mf.set_tags(tags)\r\n",
        "  mf.log_metrics(metric)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeA42UcvB6gV"
      },
      "source": [
        "## Great - Let's start the coding session!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp4V_xJzWynf"
      },
      "source": [
        "***Important:*** Before you start with your colleagues, please change the name of your experiment in the `.env` to the name of your group, such as `\"MLFLOW_EXPERIMENT=HDS-LEE-Coffee-addicts\"`. Otherwise, we may get lost in different runs<br><br>\n",
        "\n",
        "Have fun :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8frAM7GopeC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HIDA-Workshop pipelines&projects.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "724d7f4835cb671c0cd3b6a80ea8ae7681ec35122d136ff0dffb937dc8baf58d"
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}